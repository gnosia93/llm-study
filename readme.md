## Fundamentals ##

### NLP ###
* [형태소 분석기 정리](https://hipster4020.tistory.com/184)
* [Huggingface NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=tf)
* [Huggingface NLP Course - Korean](https://wikidocs.net/book/8056)
* [딥러닝을 위한 자연언어 처리 입문](https://wikidocs.net/book/2155)
* [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)
* [워드 임베딩의 한계 / 문장 임베딩 / Bidirectional Model / Attention / BERT (김도형 연구원)](https://www.youtube.com/watch?v=F10Ii6x2y3Q)
* [BERT 자세히 들여다보기 / Hugging Face / Tensorboard / Pytorch Lightening / BERT를 사용하여 환경 데이터 분류하기 (김도형 연구원)
](https://www.youtube.com/watch?v=wRMOO9uc6do)
* https://github.com/SKTBrain/KoBERT

### Transformer ###
  * [Transformer 강의 1 - Attention 설명](https://www.youtube.com/watch?v=kyIw0nHoG9w)    
  * [[챗GPT 러닝데이] Transformer 모델 개요와 GPT3 모델 활용 실습](https://www.youtube.com/watch?v=uzcRCmg9hlc)
  * [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
   
### RAG ###

  * https://velog.io/@tobigs-nlp/Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks
  * https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1  
  * [Augmenting Large Language Models with Verified Information Sources: Leveraging AWS SageMaker and OpenSearch for Knowledge-Driven Question Answering](https://medium.com/@shankar.arunp/augmenting-large-language-models-with-verified-information-sources-leveraging-aws-sagemaker-and-f6be17fb10a8)
  * [Vector Database (feat. Pinecone)](https://velog.io/@tura/vector-databases) 
  

### Quantization / PEFT ###
  - [딥러닝 Quantization(양자화) 정리](https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC)
  - [Mixed Precision - BF16의 특징과 장단점](https://thecho7.tistory.com/entry/Mixed-Precision-BF16%EC%9D%98-%ED%8A%B9%EC%A7%95%EA%B3%BC-%EC%9E%A5%EB%8B%A8%EC%A0%90)
  - [딥러닝모델에서의 양자화 - 정태희 박사(AMD)](https://www.youtube.com/watch?v=91_hhex0CmU)
  - [Quantize 🤗 Transformers models](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)
  - [Understanding 4bit Quantization: QLoRA explained (w/ Colab)](https://www.youtube.com/watch?v=TPcXVJ1VSRI)
  - [PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU](https://www.youtube.com/watch?v=YVU5wAA6Txo)   <---
  - [Boost Fine-Tuning Performance of LLM: Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU](https://www.youtube.com/watch?v=A-a-l_sFtYM)
  - https://docs.adapterhub.ml/
  - Microsoft DeepSpeed introduction at KAUST - https://www.youtube.com/watch?v=wbG2ZEDPIyw
  - PyTorch 양자화 심층 분석 - Chris Gottbrath - https://www.youtube.com/watch?v=c3MT2qV5f9w
  - [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
  - [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration) 
  * LoRA
    * [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://www.youtube.com/watch?v=BJqwmDpa0wM)
    * [[챗GPT 러닝데이 | 챗GPT말고 LLM] LoRA로 빠르게 나만의 모델을 만들어보자 - 김용담](https://www.youtube.com/watch?v=66GD0Bj5Whk)
    * https://github.com/microsoft/LoRA
    * https://github.com/huggingface/peft 
  * LudWig
    * [Efficient Fine-Tuning for Llama-v2-7b on a Single GPU](https://www.youtube.com/watch?v=g68qlo9Izf0)
    * https://github.com/ludwig-ai/ludwig/tree/master 


### LLM & LangChain ###

* [🦜️ LangChain + Streamlit🔥+ Llama 🦙: Bringing Conversational AI to Your Local Machine 🤯](https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172)
* [프롬프트 엔지니어링 가이드](https://www.promptingguide.ai/kr)
* [langchain-how-to-create-custom-knowledge-chatbots](https://www.freecodecamp.org/news/langchain-how-to-create-custom-knowledge-chatbots/)
* https://lablab.ai/t/ai-agents-tutorial-how-to-use-and-create-them

### MLPerf ###

  * https://github.com/mlcommons/inference

    
## 분산학습 ##

* [딥러닝 분산학습](https://lifeisenjoyable.tistory.com/21)
* [[챗GPT 러닝데이 | 챗GPT말고 LLM] 딥러닝 병렬처리 및 Polyglot 언어모델](https://www.youtube.com/watch?v=a0TB-_WFjNk)
* [C++ 병렬처리](https://m.blog.naver.com/PostView.naver?blogId=enter_maintanance&logNo=221830860742&categoryNo=17&proxyReferer=)
* [A friendly introduction to distributed training (ML Tech Talks)](https://www.youtube.com/watch?v=S1tN9a4Proc)
#### Pytorch DDP ####
* [Part 1: Welcome to the Distributed Data Parallel (DDP) Tutorial Series](https://www.youtube.com/watch?v=-K3bZYHYHEA&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj)
* [Part 2: What is Distributed Data Parallel (DDP)](https://www.youtube.com/watch?v=Cvdhwx-OBBo&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj&index=2)  




## Fine Tunning ##
![](https://github.com/gnosia93/llm-study/blob/main/vast-ai.png)

* [Fine-tuning Llama 2 on Your Own Dataset](https://www.google.com/search?q=llm+fine+tunning&rlz=1C5GCEM_enKR1026KR1026&oq=LLM+fine&gs_lcrp=EgZjaHJvbWUqBggBEEUYOzIGCAAQRRg5MgYIARBFGDsyBggCEEUYPDIGCAMQRRg8MgYIBBBFGDzSAQk3MTU1ajBqMTWoAgCwAgA&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:7393269a,vid:MDA3LUKNl1E,st:0)
* [Q: How to create an Instruction Dataset for Fine-tuning my LLM?](https://www.youtube.com/watch?v=BJQrQT2Xfyo)
* [모두를 위한 대규모 언어 모델 LLM Part 1 - Llama 2 Fine-Tuning 해보기](https://www.udemy.com/course/llm-part-1-llama-2-fine-tuning/)
      
#### LLaMA ####
* [메타의 언어모델, LLaMA 라마 코드분석](https://www.youtube.com/watch?v=jvYpv9VJBOA)
* [RLHF 코드리뷰 feat ChatLLaMA😎](https://www.youtube.com/watch?v=T1XadeiKl1M)
#### Alpaca - https://github.com/tatsu-lab/stanford_alpaca ####
* [스탠포드 알파카 Stanford Alpaca 코드분석 - 학습데이터 생성(Self Instruct)](https://www.youtube.com/watch?v=dLo4QkEq-Hg)
* [스탠포드 알파카 Stanford Alpaca 코드분석 - 파인튜닝](https://www.youtube.com/watch?v=u2tQYgrLouo)
#### KoAlpaca ####
* [명령어를 이해하는 오픈소스 언어 모델 ‘KoAlpaca’ 개발기](https://www.youtube.com/watch?v=7HbugcCBXwE)


## 블로그 / Youtube ##
* [Meta AI에서 개발한 ChatGPT의 대항마, LLaMA](https://devocean.sk.com/blog/techBoardDetail.do?ID=164601)
* [ChatGPT, LLaMA 그리고 이젠 Alpaca?](https://devocean.sk.com/blog/techBoardDetail.do?ID=164659)
* [거대언어 모델의 프롬프트 데이터](https://ncsoft.github.io/ncresearch/3147b0357afb32f7da8b67f2f76d6d626813f38b)
* [Microsoft Startup Summit 2023](https://www.youtube.com/playlist?list=PLGh_JNxzXsX9NSm-iyAdS4Ioco0vp4jtq)
* [😎경량화! 노트북에 구축하는 대규모 언어모델 GPT4All😎 (feat, ChatGPT, GPT3.5, LLaMA, Alpaca, Vicuna)](https://www.youtube.com/watch?v=HewtI35-lp8)
* [[Paper Review] Democratizing Large Language Models : From 175B to 7B](https://www.youtube.com/watch?v=ORYQU0RYn_M)


## Diffusion Model ##

* [[Paper Review] Denoising Diffusion Probabilistic Models](https://www.youtube.com/watch?v=_JQSMhqXw-4)
* [Diffusion Model 수학이 포함된 tutorial](https://www.youtube.com/watch?v=uFoGaIVHfoE)
* [Diffusion Model 설명 – 기초부터 응용까지](https://ffighting.net/deep-learning-paper-review/diffusion-model/diffusion-model-basic/)


## Paper Review ##

* [[17′ NIPS] Transformer : Attention Is All You Need](https://ffighting.net/deep-learning-paper-review/language-model/transformer/)
* [[20′ NIPS] GPT-3 : Language Models are Few-Shot Learners](https://ffighting.net/deep-learning-paper-review/language-model/gpt-3/)
* [https://paperswithcode.com/sota](https://paperswithcode.com/sota)
  
## 참고 사이트 ##
* 한국인공지능 아카데미 - https://www.youtube.com/watch?v=vziygFrRlZ4
* https://www.youtube.com/@code4AI/featured  
* https://www.youtube.com/@samwitteveenai
* https://www.youtube.com/@ShawhinTalebi/featured
* https://www.youtube.com/@practical-ai
* 인공지능 개발자 모임 - http://aidev.co.kr/
* https://civitai.com/          --- LoRA + stable difussion + GPU 8G 이상. 
* 인공지능 팩토리 - https://www.youtube.com/@aifactoryspace
