## ML Fundamentals ##

### ì„ë² ë”© ###
  * [ì„ë² ë”©ì´ë€](https://today-gaze-697915.framer.app/ko/blog/what-is-embedding-and-how-to-use?fbclid=IwAR0_nBjBHq3I96SmbqN0mVLj_rnlbo-YaY6N3kN3yIz8e1DzUuYl6TmDBkw)
  * [ì›Œë“œ ì„ë² ë”©ì˜ í•œê³„ / ë¬¸ì¥ ì„ë² ë”© / Bidirectional Model / Attention / BERT (ê¹€ë„í˜• ì—°êµ¬ì›)](https://www.youtube.com/watch?v=F10Ii6x2y3Q)
  * [BERT ìì„¸íˆ ë“¤ì—¬ë‹¤ë³´ê¸° / Hugging Face / Tensorboard / Pytorch Lightening / BERTë¥¼ ì‚¬ìš©í•˜ì—¬ í™˜ê²½ ë°ì´í„° ë¶„ë¥˜í•˜ê¸° (ê¹€ë„í˜• ì—°êµ¬ì›)
](https://www.youtube.com/watch?v=wRMOO9uc6do)
  * https://github.com/SKTBrain/KoBERT

### NLP ###
* [í˜•íƒœì†Œ ë¶„ì„ê¸° ì •ë¦¬](https://hipster4020.tistory.com/184)
* [Huggingface NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=tf)
* [Huggingface NLP Course - Korean](https://wikidocs.net/book/8056)
* [ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìì—°ì–¸ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/book/2155)
* [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)


### Transformer ###
  * [Transformer ê°•ì˜ 1 - Attention ì„¤ëª…](https://www.youtube.com/watch?v=kyIw0nHoG9w)    
  * [[ì±—GPT ëŸ¬ë‹ë°ì´] Transformer ëª¨ë¸ ê°œìš”ì™€ GPT3 ëª¨ë¸ í™œìš© ì‹¤ìŠµ](https://www.youtube.com/watch?v=uzcRCmg9hlc)
  * [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
   
### RAG ###

  * https://velog.io/@tobigs-nlp/Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks
  * https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1  
  * [Augmenting Large Language Models with Verified Information Sources: Leveraging AWS SageMaker and OpenSearch for Knowledge-Driven Question Answering](https://medium.com/@shankar.arunp/augmenting-large-language-models-with-verified-information-sources-leveraging-aws-sagemaker-and-f6be17fb10a8)

  - [Vector Database (feat. Pinecone)](https://velog.io/@tura/vector-databases) 
  - https://devocean.sk.com/blog/techBoardDetail.do?ID=164964&boardType=techBlog
  - https://unzip.dev/0x014-vector-databases/
  

### Quantization / PEFT ###
  - [ë”¥ëŸ¬ë‹ Quantization(ì–‘ìí™”) ì •ë¦¬](https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC)
  - [Mixed Precision - BF16ì˜ íŠ¹ì§•ê³¼ ì¥ë‹¨ì ](https://thecho7.tistory.com/entry/Mixed-Precision-BF16%EC%9D%98-%ED%8A%B9%EC%A7%95%EA%B3%BC-%EC%9E%A5%EB%8B%A8%EC%A0%90)
  - [ë”¥ëŸ¬ë‹ëª¨ë¸ì—ì„œì˜ ì–‘ìí™” - ì •íƒœí¬ ë°•ì‚¬(AMD)](https://www.youtube.com/watch?v=91_hhex0CmU)
  - [Quantize ğŸ¤— Transformers models](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)
  - [Understanding 4bit Quantization: QLoRA explained (w/ Colab)](https://www.youtube.com/watch?v=TPcXVJ1VSRI)
  - [PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU](https://www.youtube.com/watch?v=YVU5wAA6Txo)   <---
  - [Boost Fine-Tuning Performance of LLM: Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU](https://www.youtube.com/watch?v=A-a-l_sFtYM)
  - https://docs.adapterhub.ml/
  - Microsoft DeepSpeed introduction at KAUST - https://www.youtube.com/watch?v=wbG2ZEDPIyw
  - PyTorch ì–‘ìí™” ì‹¬ì¸µ ë¶„ì„ - Chris Gottbrath - https://www.youtube.com/watch?v=c3MT2qV5f9w
  - [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
  - [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration) 
  * LoRA
    * [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://www.youtube.com/watch?v=BJqwmDpa0wM)
    * [[ì±—GPT ëŸ¬ë‹ë°ì´ | ì±—GPTë§ê³  LLM] LoRAë¡œ ë¹ ë¥´ê²Œ ë‚˜ë§Œì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì - ê¹€ìš©ë‹´](https://www.youtube.com/watch?v=66GD0Bj5Whk)
    * https://github.com/microsoft/LoRA
    * https://github.com/huggingface/peft 
  * LudWig
    * [Efficient Fine-Tuning for Llama-v2-7b on a Single GPU](https://www.youtube.com/watch?v=g68qlo9Izf0)
    * https://github.com/ludwig-ai/ludwig/tree/master 


### MLOps ###
  * [2020 ìºê¸€ëŸ¬ë‹ë°ì´ - mlflowá„…á…© á„€á…¡á†«á„ƒá…¡á†«á„’á…¡á†« MLOps á„€á…®á„á…®á†¨á„’á…¡á„€á…µ](https://www.youtube.com/watch?v=OB9vbJr8XdQ)
  * [MLflowë¥¼ ì´ìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ ê´€ë¦¬. ë°•ì„ ì¤€- PyCon Korea 2021](https://www.youtube.com/watch?v=H-4ZIfOJDaw)
  * [ì¹´ì¹´ì˜¤ë±…í¬ì˜ MLOps / if(kakao)dev2022](https://www.youtube.com/watch?v=Fj0MOkzCECA)
  * [MLOps on Databricks: A How-To Guide](https://www.youtube.com/watch?v=JApPzAnbfPI)
  * [ì¹´ì¹´ì˜¤í˜ì´ MLOps ì ìš©ê¸° (feat. AWS) / if(kakao)2022](https://www.youtube.com/watch?v=5FvTXzDLPxI)
  * [Validation Techniques-ML](https://trsekhar123.medium.com/validation-techniques-ml-a984fa98cbd6)
  * [Various ways to evaluate a machine learning modelâ€™s performance](https://towardsdatascience.com/various-ways-to-evaluate-a-machine-learning-models-performance-230449055f15)
  * [í”¼ì²˜ìŠ¤í† ì–´](https://medium.com/data-for-ai/what-is-a-feature-store-for-ml-29b62580af5d)
    
### ë¶„ì‚°í•™ìŠµ ###

  * [ë”¥ëŸ¬ë‹ ë¶„ì‚°í•™ìŠµ](https://lifeisenjoyable.tistory.com/21)
  * [[ì±—GPT ëŸ¬ë‹ë°ì´ | ì±—GPTë§ê³  LLM] ë”¥ëŸ¬ë‹ ë³‘ë ¬ì²˜ë¦¬ ë° Polyglot ì–¸ì–´ëª¨ë¸](https://www.youtube.com/watch?v=a0TB-_WFjNk)
  * [C++ ë³‘ë ¬ì²˜ë¦¬](https://m.blog.naver.com/PostView.naver?blogId=enter_maintanance&logNo=221830860742&categoryNo=17&proxyReferer=)
    
### MLPerf ###

  * https://github.com/mlcommons/inference


### í†µê³„í•™ ###

* [ì´ì‚°í™•ë¥ ë¶„í¬](https://m.blog.naver.com/algosn/221288016739)


## Fine Tunning ##
![](https://github.com/gnosia93/llm-study/blob/main/vast-ai.png)

* [Fine-tuning Llama 2 on Your Own Dataset](https://www.google.com/search?q=llm+fine+tunning&rlz=1C5GCEM_enKR1026KR1026&oq=LLM+fine&gs_lcrp=EgZjaHJvbWUqBggBEEUYOzIGCAAQRRg5MgYIARBFGDsyBggCEEUYPDIGCAMQRRg8MgYIBBBFGDzSAQk3MTU1ajBqMTWoAgCwAgA&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:7393269a,vid:MDA3LUKNl1E,st:0)
* [Q: How to create an Instruction Dataset for Fine-tuning my LLM?](https://www.youtube.com/watch?v=BJQrQT2Xfyo)
* [ëª¨ë‘ë¥¼ ìœ„í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ LLM Part 1 - Llama 2 Fine-Tuning í•´ë³´ê¸°](https://www.udemy.com/course/llm-part-1-llama-2-fine-tuning/)
      
#### Alpaca - https://github.com/tatsu-lab/stanford_alpaca ####
* [ìŠ¤íƒ í¬ë“œ ì•ŒíŒŒì¹´ Stanford Alpaca ì½”ë“œë¶„ì„ - í•™ìŠµë°ì´í„° ìƒì„±(Self Instruct)](https://www.youtube.com/watch?v=dLo4QkEq-Hg)
* [ìŠ¤íƒ í¬ë“œ ì•ŒíŒŒì¹´ Stanford Alpaca ì½”ë“œë¶„ì„ - íŒŒì¸íŠœë‹](https://www.youtube.com/watch?v=u2tQYgrLouo)




## LLM & LangChain ##

* [ğŸ¦œï¸ LangChain + StreamlitğŸ”¥+ Llama ğŸ¦™: Bringing Conversational AI to Your Local Machine ğŸ¤¯](https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172)
* [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œ](https://www.promptingguide.ai/kr)
* [langchain-how-to-create-custom-knowledge-chatbots](https://www.freecodecamp.org/news/langchain-how-to-create-custom-knowledge-chatbots/)
* https://lablab.ai/t/ai-agents-tutorial-how-to-use-and-create-them



## AWS / SageMaker / Huggingface ##

* https://huggingface.co/blog/sagemaker-huggingface-llm
* [Deep Learning Container Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)
* [Fundamentals of combining LangChain and Amazon SageMaker (with Llama 2 Example)](https://medium.com/@ryanlempka/fundamentals-of-combining-langchain-and-sagemaker-with-a-llama-2-example-694924ab0d92)
* https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/
* https://github.com/aws-samples/aws-genai-llm-chatbot/#deployment-dependencies-installation
* https://huggingface.co/blog/getting-started-with-embeddings
* [Huggingface í—ˆê¹…í˜ì´ìŠ¤ íŒŒí—¤ì¹˜ê¸°](https://hipster4020.tistory.com/172)
* [Huggingface í—ˆê¹…í˜ì´ìŠ¤ - NLP](https://hipster4020.tistory.com/176)
* https://github.com/huggingface
  

## ChatBot ##

* https://dev.to/aws/building-an-aws-well-architected-chatbot-with-langchain-13cd
* https://github.com/aws-samples/aws-genai-llm-chatbot
* https://itnext.io/building-a-multi-turn-chatbot-with-gpt-and-sagemaker-a-step-by-step-guide-7d75f33ccea1
* https://aws.amazon.com/ko/blogs/tech/sagemaker-jumpstart-vector-store-llama2-chatbot/
* [ë”¥ëŸ¬ë‹ ì±—ë´‡ ììŠµì„œ](https://hashdork.com/ko/create-a-deep-learning-chatbot-with-python/)
* [ì²˜ìŒë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ì±—ë´‡](https://github.com/keiraydev/chatbot)
* [GRADIO-LangChain](https://velog.io/@t_wave/gradiolangchainchatbot)
* [Bedrock ë´‡?](https://github.com/seungwon2/Bedrock-emotional-cs-bot)


## ì½”ë“œ ë¶„ì„ ##

* [ChatGPTë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë›°ì–´ë‚˜ë‹¤? _ ë©”íƒ€ì˜ ì–¸ì–´ëª¨ë¸, LLaMA ë¼ë§ˆ ì½”ë“œë¶„ì„](https://www.youtube.com/watch?v=jvYpv9VJBOA)


* [ğŸ˜ChatGPT í•µì‹¬ê¸°ìˆ  RLHF ì½”ë“œë¦¬ë·° feat ChatLLaMAğŸ˜](https://www.youtube.com/watch?v=T1XadeiKl1M)
* [Microsoft Startup Summit 2023](https://www.youtube.com/playlist?list=PLGh_JNxzXsX9NSm-iyAdS4Ioco0vp4jtq)

## ë¸”ë¡œê·¸ s ##
* [Meta AIì—ì„œ ê°œë°œí•œ ChatGPTì˜ ëŒ€í•­ë§ˆ, LLaMA](https://devocean.sk.com/blog/techBoardDetail.do?ID=164601)
* [ChatGPT, LLaMA ê·¸ë¦¬ê³  ì´ì   Alpaca?](https://devocean.sk.com/blog/techBoardDetail.do?ID=164659)
* [ê±°ëŒ€ì–¸ì–´ ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ ë°ì´í„°](https://ncsoft.github.io/ncresearch/3147b0357afb32f7da8b67f2f76d6d626813f38b)
  
## ì°¸ê³  ì‚¬ì´íŠ¸ ##
* https://www.youtube.com/@code4AI/featured   <--- ê°€ì¥ ì¢‹ì€ ë“¯...
* í•œêµ­ì¸ê³µì§€ëŠ¥ ì•„ì¹´ë°ë¯¸ - https://www.youtube.com/watch?v=vziygFrRlZ4
* https://www.youtube.com/@samwitteveenai
* https://www.youtube.com/@ShawhinTalebi/featured
* https://www.youtube.com/@practical-ai
* ì¸ê³µì§€ëŠ¥ ê°œë°œì ëª¨ì„ - http://aidev.co.kr/
* https://civitai.com/          --- LoRA + stable difussion + GPU 8G ì´ìƒ. 
* ì¸ê³µì§€ëŠ¥ íŒ©í† ë¦¬ - https://www.youtube.com/@aifactoryspace
