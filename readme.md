## ML Fundamentals ##

### 임베딩 ###
  * [임베딩이란](https://today-gaze-697915.framer.app/ko/blog/what-is-embedding-and-how-to-use?fbclid=IwAR0_nBjBHq3I96SmbqN0mVLj_rnlbo-YaY6N3kN3yIz8e1DzUuYl6TmDBkw)
  * [워드 임베딩의 한계 / 문장 임베딩 / Bidirectional Model / Attention / BERT (김도형 연구원)](https://www.youtube.com/watch?v=F10Ii6x2y3Q)
  * [BERT 자세히 들여다보기 / Hugging Face / Tensorboard / Pytorch Lightening / BERT를 사용하여 환경 데이터 분류하기 (김도형 연구원)
](https://www.youtube.com/watch?v=wRMOO9uc6do)
  * https://github.com/SKTBrain/KoBERT

### NLP ###
* [형태소 분석기 정리](https://hipster4020.tistory.com/184)
* [Huggingface NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=tf)
* [Huggingface NLP Course - Korean](https://wikidocs.net/book/8056)
* [딥러닝을 위한 자연언어 처리 입문](https://wikidocs.net/book/2155)
* [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)


### Transformer ###
  * [Transformer 강의 1 - Attention 설명](https://www.youtube.com/watch?v=kyIw0nHoG9w)    
  * [[챗GPT 러닝데이] Transformer 모델 개요와 GPT3 모델 활용 실습](https://www.youtube.com/watch?v=uzcRCmg9hlc)
  * [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
   
### RAG ###

  * https://velog.io/@tobigs-nlp/Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks
  * https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1  
  * [Augmenting Large Language Models with Verified Information Sources: Leveraging AWS SageMaker and OpenSearch for Knowledge-Driven Question Answering](https://medium.com/@shankar.arunp/augmenting-large-language-models-with-verified-information-sources-leveraging-aws-sagemaker-and-f6be17fb10a8)

  - [Vector Database (feat. Pinecone)](https://velog.io/@tura/vector-databases) 
  - https://devocean.sk.com/blog/techBoardDetail.do?ID=164964&boardType=techBlog
  - https://unzip.dev/0x014-vector-databases/
  

### Quantization / PEFT ###
  - [딥러닝 Quantization(양자화) 정리](https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC)
  - [Mixed Precision - BF16의 특징과 장단점](https://thecho7.tistory.com/entry/Mixed-Precision-BF16%EC%9D%98-%ED%8A%B9%EC%A7%95%EA%B3%BC-%EC%9E%A5%EB%8B%A8%EC%A0%90)
  - [딥러닝모델에서의 양자화 - 정태희 박사(AMD)](https://www.youtube.com/watch?v=91_hhex0CmU)
  - [Quantize 🤗 Transformers models](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)
  - [Understanding 4bit Quantization: QLoRA explained (w/ Colab)](https://www.youtube.com/watch?v=TPcXVJ1VSRI)
  - [PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU](https://www.youtube.com/watch?v=YVU5wAA6Txo)   <---
  - [Boost Fine-Tuning Performance of LLM: Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU](https://www.youtube.com/watch?v=A-a-l_sFtYM)
  - https://docs.adapterhub.ml/
  - Microsoft DeepSpeed introduction at KAUST - https://www.youtube.com/watch?v=wbG2ZEDPIyw
  - PyTorch 양자화 심층 분석 - Chris Gottbrath - https://www.youtube.com/watch?v=c3MT2qV5f9w
  - [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
  - [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration) 
  * LoRA
    * [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://www.youtube.com/watch?v=BJqwmDpa0wM)
    * [[챗GPT 러닝데이 | 챗GPT말고 LLM] LoRA로 빠르게 나만의 모델을 만들어보자 - 김용담](https://www.youtube.com/watch?v=66GD0Bj5Whk)
    * https://github.com/microsoft/LoRA
    * https://github.com/huggingface/peft 
  * LudWig
    * [Efficient Fine-Tuning for Llama-v2-7b on a Single GPU](https://www.youtube.com/watch?v=g68qlo9Izf0)
    * https://github.com/ludwig-ai/ludwig/tree/master 


### MLOps ###
  * [2020 캐글러닝데이 - mlflow로 간단한 MLOps 구축하기](https://www.youtube.com/watch?v=OB9vbJr8XdQ)
  * [MLflow를 이용한 머신러닝 프로젝트 관리. 박선준- PyCon Korea 2021](https://www.youtube.com/watch?v=H-4ZIfOJDaw)
  * [카카오뱅크의 MLOps / if(kakao)dev2022](https://www.youtube.com/watch?v=Fj0MOkzCECA)
  * [MLOps on Databricks: A How-To Guide](https://www.youtube.com/watch?v=JApPzAnbfPI)
  * [카카오페이 MLOps 적용기 (feat. AWS) / if(kakao)2022](https://www.youtube.com/watch?v=5FvTXzDLPxI)
  * [Validation Techniques-ML](https://trsekhar123.medium.com/validation-techniques-ml-a984fa98cbd6)
  * [Various ways to evaluate a machine learning model’s performance](https://towardsdatascience.com/various-ways-to-evaluate-a-machine-learning-models-performance-230449055f15)
  * [피처스토어](https://medium.com/data-for-ai/what-is-a-feature-store-for-ml-29b62580af5d)
    
### 분산학습 ###

  * [딥러닝 분산학습](https://lifeisenjoyable.tistory.com/21)
  * [[챗GPT 러닝데이 | 챗GPT말고 LLM] 딥러닝 병렬처리 및 Polyglot 언어모델](https://www.youtube.com/watch?v=a0TB-_WFjNk)
  * [C++ 병렬처리](https://m.blog.naver.com/PostView.naver?blogId=enter_maintanance&logNo=221830860742&categoryNo=17&proxyReferer=)
    
### MLPerf ###

  * https://github.com/mlcommons/inference


### 통계학 ###

* [이산확률분포](https://m.blog.naver.com/algosn/221288016739)


## Fine Tunning ##
![](https://github.com/gnosia93/llm-study/blob/main/vast-ai.png)

* [Fine-tuning Llama 2 on Your Own Dataset](https://www.google.com/search?q=llm+fine+tunning&rlz=1C5GCEM_enKR1026KR1026&oq=LLM+fine&gs_lcrp=EgZjaHJvbWUqBggBEEUYOzIGCAAQRRg5MgYIARBFGDsyBggCEEUYPDIGCAMQRRg8MgYIBBBFGDzSAQk3MTU1ajBqMTWoAgCwAgA&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:7393269a,vid:MDA3LUKNl1E,st:0)
* [Q: How to create an Instruction Dataset for Fine-tuning my LLM?](https://www.youtube.com/watch?v=BJQrQT2Xfyo)
* [모두를 위한 대규모 언어 모델 LLM Part 1 - Llama 2 Fine-Tuning 해보기](https://www.udemy.com/course/llm-part-1-llama-2-fine-tuning/)
      
#### Alpaca - https://github.com/tatsu-lab/stanford_alpaca ####
* [스탠포드 알파카 Stanford Alpaca 코드분석 - 학습데이터 생성(Self Instruct)](https://www.youtube.com/watch?v=dLo4QkEq-Hg)
* [스탠포드 알파카 Stanford Alpaca 코드분석 - 파인튜닝](https://www.youtube.com/watch?v=u2tQYgrLouo)




## LLM & LangChain ##

* [🦜️ LangChain + Streamlit🔥+ Llama 🦙: Bringing Conversational AI to Your Local Machine 🤯](https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172)
* [프롬프트 엔지니어링 가이드](https://www.promptingguide.ai/kr)
* [langchain-how-to-create-custom-knowledge-chatbots](https://www.freecodecamp.org/news/langchain-how-to-create-custom-knowledge-chatbots/)
* https://lablab.ai/t/ai-agents-tutorial-how-to-use-and-create-them



## AWS / SageMaker / Huggingface ##

* https://huggingface.co/blog/sagemaker-huggingface-llm
* [Deep Learning Container Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)
* [Fundamentals of combining LangChain and Amazon SageMaker (with Llama 2 Example)](https://medium.com/@ryanlempka/fundamentals-of-combining-langchain-and-sagemaker-with-a-llama-2-example-694924ab0d92)
* https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/
* https://github.com/aws-samples/aws-genai-llm-chatbot/#deployment-dependencies-installation
* https://huggingface.co/blog/getting-started-with-embeddings
* [Huggingface 허깅페이스 파헤치기](https://hipster4020.tistory.com/172)
* [Huggingface 허깅페이스 - NLP](https://hipster4020.tistory.com/176)
* https://github.com/huggingface
  

## ChatBot ##

* https://dev.to/aws/building-an-aws-well-architected-chatbot-with-langchain-13cd
* https://github.com/aws-samples/aws-genai-llm-chatbot
* https://itnext.io/building-a-multi-turn-chatbot-with-gpt-and-sagemaker-a-step-by-step-guide-7d75f33ccea1
* https://aws.amazon.com/ko/blogs/tech/sagemaker-jumpstart-vector-store-llama2-chatbot/
* [딥러닝 챗봇 자습서](https://hashdork.com/ko/create-a-deep-learning-chatbot-with-python/)
* [처음배우는 딥러닝 챗봇](https://github.com/keiraydev/chatbot)
* [GRADIO-LangChain](https://velog.io/@t_wave/gradiolangchainchatbot)
* [Bedrock 봇?](https://github.com/seungwon2/Bedrock-emotional-cs-bot)


## 코드 분석 ##

* [ChatGPT보다 성능이 더 뛰어나다? _ 메타의 언어모델, LLaMA 라마 코드분석](https://www.youtube.com/watch?v=jvYpv9VJBOA)


* [😎ChatGPT 핵심기술 RLHF 코드리뷰 feat ChatLLaMA😎](https://www.youtube.com/watch?v=T1XadeiKl1M)
* [Microsoft Startup Summit 2023](https://www.youtube.com/playlist?list=PLGh_JNxzXsX9NSm-iyAdS4Ioco0vp4jtq)

## 블로그 s ##
* [Meta AI에서 개발한 ChatGPT의 대항마, LLaMA](https://devocean.sk.com/blog/techBoardDetail.do?ID=164601)
* [ChatGPT, LLaMA 그리고 이젠 Alpaca?](https://devocean.sk.com/blog/techBoardDetail.do?ID=164659)
* [거대언어 모델의 프롬프트 데이터](https://ncsoft.github.io/ncresearch/3147b0357afb32f7da8b67f2f76d6d626813f38b)
  
## 참고 사이트 ##
* https://www.youtube.com/@code4AI/featured   <--- 가장 좋은 듯...
* 한국인공지능 아카데미 - https://www.youtube.com/watch?v=vziygFrRlZ4
* https://www.youtube.com/@samwitteveenai
* https://www.youtube.com/@ShawhinTalebi/featured
* https://www.youtube.com/@practical-ai
* 인공지능 개발자 모임 - http://aidev.co.kr/
* https://civitai.com/          --- LoRA + stable difussion + GPU 8G 이상. 
* 인공지능 팩토리 - https://www.youtube.com/@aifactoryspace
