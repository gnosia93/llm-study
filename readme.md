## Fundamentals ##

### NLP ###
* [í˜•íƒœì†Œ ë¶„ì„ê¸° ì •ë¦¬](https://hipster4020.tistory.com/184)
* [Huggingface NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=tf)
* [Huggingface NLP Course - Korean](https://wikidocs.net/book/8056)
* [ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìì—°ì–¸ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/book/2155)
* [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)
* [ì›Œë“œ ì„ë² ë”©ì˜ í•œê³„ / ë¬¸ì¥ ì„ë² ë”© / Bidirectional Model / Attention / BERT (ê¹€ë„í˜• ì—°êµ¬ì›)](https://www.youtube.com/watch?v=F10Ii6x2y3Q)
* [BERT ìì„¸íˆ ë“¤ì—¬ë‹¤ë³´ê¸° / Hugging Face / Tensorboard / Pytorch Lightening / BERTë¥¼ ì‚¬ìš©í•˜ì—¬ í™˜ê²½ ë°ì´í„° ë¶„ë¥˜í•˜ê¸° (ê¹€ë„í˜• ì—°êµ¬ì›)
](https://www.youtube.com/watch?v=wRMOO9uc6do)
* https://github.com/SKTBrain/KoBERT

### Transformer ###
  * [Transformer ê°•ì˜ 1 - Attention ì„¤ëª…](https://www.youtube.com/watch?v=kyIw0nHoG9w)    
  * [[ì±—GPT ëŸ¬ë‹ë°ì´] Transformer ëª¨ë¸ ê°œìš”ì™€ GPT3 ëª¨ë¸ í™œìš© ì‹¤ìŠµ](https://www.youtube.com/watch?v=uzcRCmg9hlc)
  * [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
   
### RAG ###

  * https://velog.io/@tobigs-nlp/Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks
  * https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1  
  * [Augmenting Large Language Models with Verified Information Sources: Leveraging AWS SageMaker and OpenSearch for Knowledge-Driven Question Answering](https://medium.com/@shankar.arunp/augmenting-large-language-models-with-verified-information-sources-leveraging-aws-sagemaker-and-f6be17fb10a8)
  * [Vector Database (feat. Pinecone)](https://velog.io/@tura/vector-databases) 
  

### Quantization / PEFT ###
  - [ë”¥ëŸ¬ë‹ Quantization(ì–‘ìí™”) ì •ë¦¬](https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC)
  - [Mixed Precision - BF16ì˜ íŠ¹ì§•ê³¼ ì¥ë‹¨ì ](https://thecho7.tistory.com/entry/Mixed-Precision-BF16%EC%9D%98-%ED%8A%B9%EC%A7%95%EA%B3%BC-%EC%9E%A5%EB%8B%A8%EC%A0%90)
  - [ë”¥ëŸ¬ë‹ëª¨ë¸ì—ì„œì˜ ì–‘ìí™” - ì •íƒœí¬ ë°•ì‚¬(AMD)](https://www.youtube.com/watch?v=91_hhex0CmU)
  - [Quantize ğŸ¤— Transformers models](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)
  - [Understanding 4bit Quantization: QLoRA explained (w/ Colab)](https://www.youtube.com/watch?v=TPcXVJ1VSRI)
  - [PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU](https://www.youtube.com/watch?v=YVU5wAA6Txo)   <---
  - [Boost Fine-Tuning Performance of LLM: Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU](https://www.youtube.com/watch?v=A-a-l_sFtYM)
  - https://docs.adapterhub.ml/
  - Microsoft DeepSpeed introduction at KAUST - https://www.youtube.com/watch?v=wbG2ZEDPIyw
  - PyTorch ì–‘ìí™” ì‹¬ì¸µ ë¶„ì„ - Chris Gottbrath - https://www.youtube.com/watch?v=c3MT2qV5f9w
  - [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
  - [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration) 
  * LoRA
    * [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://www.youtube.com/watch?v=BJqwmDpa0wM)
    * [[ì±—GPT ëŸ¬ë‹ë°ì´ | ì±—GPTë§ê³  LLM] LoRAë¡œ ë¹ ë¥´ê²Œ ë‚˜ë§Œì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì - ê¹€ìš©ë‹´](https://www.youtube.com/watch?v=66GD0Bj5Whk)
    * https://github.com/microsoft/LoRA
    * https://github.com/huggingface/peft 
  * LudWig
    * [Efficient Fine-Tuning for Llama-v2-7b on a Single GPU](https://www.youtube.com/watch?v=g68qlo9Izf0)
    * https://github.com/ludwig-ai/ludwig/tree/master 


### LLM & LangChain ###

* [ğŸ¦œï¸ LangChain + StreamlitğŸ”¥+ Llama ğŸ¦™: Bringing Conversational AI to Your Local Machine ğŸ¤¯](https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172)
* [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œ](https://www.promptingguide.ai/kr)
* [langchain-how-to-create-custom-knowledge-chatbots](https://www.freecodecamp.org/news/langchain-how-to-create-custom-knowledge-chatbots/)
* https://lablab.ai/t/ai-agents-tutorial-how-to-use-and-create-them

### MLPerf ###

  * https://github.com/mlcommons/inference

    
## ë¶„ì‚°í•™ìŠµ ##

* [ë”¥ëŸ¬ë‹ ë¶„ì‚°í•™ìŠµ](https://lifeisenjoyable.tistory.com/21)
* [[ì±—GPT ëŸ¬ë‹ë°ì´ | ì±—GPTë§ê³  LLM] ë”¥ëŸ¬ë‹ ë³‘ë ¬ì²˜ë¦¬ ë° Polyglot ì–¸ì–´ëª¨ë¸](https://www.youtube.com/watch?v=a0TB-_WFjNk)
* [C++ ë³‘ë ¬ì²˜ë¦¬](https://m.blog.naver.com/PostView.naver?blogId=enter_maintanance&logNo=221830860742&categoryNo=17&proxyReferer=)
* [A friendly introduction to distributed training (ML Tech Talks)](https://www.youtube.com/watch?v=S1tN9a4Proc)
#### Pytorch DDP ####
* [Part 1: Welcome to the Distributed Data Parallel (DDP) Tutorial Series](https://www.youtube.com/watch?v=-K3bZYHYHEA&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj)
* [Part 2: What is Distributed Data Parallel (DDP)](https://www.youtube.com/watch?v=Cvdhwx-OBBo&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj&index=2)  




## Fine Tunning ##
![](https://github.com/gnosia93/llm-study/blob/main/vast-ai.png)

* [Fine-tuning Llama 2 on Your Own Dataset](https://www.google.com/search?q=llm+fine+tunning&rlz=1C5GCEM_enKR1026KR1026&oq=LLM+fine&gs_lcrp=EgZjaHJvbWUqBggBEEUYOzIGCAAQRRg5MgYIARBFGDsyBggCEEUYPDIGCAMQRRg8MgYIBBBFGDzSAQk3MTU1ajBqMTWoAgCwAgA&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:7393269a,vid:MDA3LUKNl1E,st:0)
* [Q: How to create an Instruction Dataset for Fine-tuning my LLM?](https://www.youtube.com/watch?v=BJQrQT2Xfyo)
* [ëª¨ë‘ë¥¼ ìœ„í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ LLM Part 1 - Llama 2 Fine-Tuning í•´ë³´ê¸°](https://www.udemy.com/course/llm-part-1-llama-2-fine-tuning/)
      
#### LLaMA ####
* [ë©”íƒ€ì˜ ì–¸ì–´ëª¨ë¸, LLaMA ë¼ë§ˆ ì½”ë“œë¶„ì„](https://www.youtube.com/watch?v=jvYpv9VJBOA)
* [RLHF ì½”ë“œë¦¬ë·° feat ChatLLaMAğŸ˜](https://www.youtube.com/watch?v=T1XadeiKl1M)
#### Alpaca - https://github.com/tatsu-lab/stanford_alpaca ####
* [ìŠ¤íƒ í¬ë“œ ì•ŒíŒŒì¹´ Stanford Alpaca ì½”ë“œë¶„ì„ - í•™ìŠµë°ì´í„° ìƒì„±(Self Instruct)](https://www.youtube.com/watch?v=dLo4QkEq-Hg)
* [ìŠ¤íƒ í¬ë“œ ì•ŒíŒŒì¹´ Stanford Alpaca ì½”ë“œë¶„ì„ - íŒŒì¸íŠœë‹](https://www.youtube.com/watch?v=u2tQYgrLouo)
#### KoAlpaca ####
* [ëª…ë ¹ì–´ë¥¼ ì´í•´í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì–¸ì–´ ëª¨ë¸ â€˜KoAlpacaâ€™ ê°œë°œê¸°](https://www.youtube.com/watch?v=7HbugcCBXwE)


## ë¸”ë¡œê·¸ / Youtube ##
* [Meta AIì—ì„œ ê°œë°œí•œ ChatGPTì˜ ëŒ€í•­ë§ˆ, LLaMA](https://devocean.sk.com/blog/techBoardDetail.do?ID=164601)
* [ChatGPT, LLaMA ê·¸ë¦¬ê³  ì´ì   Alpaca?](https://devocean.sk.com/blog/techBoardDetail.do?ID=164659)
* [ê±°ëŒ€ì–¸ì–´ ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ ë°ì´í„°](https://ncsoft.github.io/ncresearch/3147b0357afb32f7da8b67f2f76d6d626813f38b)
* [Microsoft Startup Summit 2023](https://www.youtube.com/playlist?list=PLGh_JNxzXsX9NSm-iyAdS4Ioco0vp4jtq)
* [ğŸ˜ê²½ëŸ‰í™”! ë…¸íŠ¸ë¶ì— êµ¬ì¶•í•˜ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ GPT4AllğŸ˜ (feat, ChatGPT, GPT3.5, LLaMA, Alpaca, Vicuna)](https://www.youtube.com/watch?v=HewtI35-lp8)
* [[Paper Review] Democratizing Large Language Models : From 175B to 7B](https://www.youtube.com/watch?v=ORYQU0RYn_M)


## Diffusion Model ##

* [[Paper Review] Denoising Diffusion Probabilistic Models](https://www.youtube.com/watch?v=_JQSMhqXw-4)
* [Diffusion Model ìˆ˜í•™ì´ í¬í•¨ëœ tutorial](https://www.youtube.com/watch?v=uFoGaIVHfoE)
* [Diffusion Model ì„¤ëª… â€“ ê¸°ì´ˆë¶€í„° ì‘ìš©ê¹Œì§€](https://ffighting.net/deep-learning-paper-review/diffusion-model/diffusion-model-basic/)


## Paper Review ##

* [[17â€² NIPS] Transformer : Attention Is All You Need](https://ffighting.net/deep-learning-paper-review/language-model/transformer/)
* [[20â€² NIPS] GPT-3 : Language Models are Few-Shot Learners](https://ffighting.net/deep-learning-paper-review/language-model/gpt-3/)
* [https://paperswithcode.com/sota](https://paperswithcode.com/sota)
  
## ì°¸ê³  ì‚¬ì´íŠ¸ ##
* í•œêµ­ì¸ê³µì§€ëŠ¥ ì•„ì¹´ë°ë¯¸ - https://www.youtube.com/watch?v=vziygFrRlZ4
* https://www.youtube.com/@code4AI/featured  
* https://www.youtube.com/@samwitteveenai
* https://www.youtube.com/@ShawhinTalebi/featured
* https://www.youtube.com/@practical-ai
* ì¸ê³µì§€ëŠ¥ ê°œë°œì ëª¨ì„ - http://aidev.co.kr/
* https://civitai.com/          --- LoRA + stable difussion + GPU 8G ì´ìƒ. 
* ì¸ê³µì§€ëŠ¥ íŒ©í† ë¦¬ - https://www.youtube.com/@aifactoryspace
